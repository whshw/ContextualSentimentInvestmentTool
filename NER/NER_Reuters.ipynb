{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-25T15:45:33.200690Z",
     "start_time": "2023-07-25T15:45:29.847780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from   os import path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# use the first GPU if available, otherwise use CPU\n",
    "device = torch.device(\"mps\"if torch.backends.mps.is_available()else \"cpu\")\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def combineHeadlineText(row):\n",
    "    if isinstance(row[\"Headline\"], str):\n",
    "        return row[\"Headline\"] + \". \" + row[\"Text\"]\n",
    "    else:\n",
    "        return row[\"Text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T15:45:33.202678Z",
     "start_time": "2023-07-25T15:45:33.201099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def process_entities(ner_results):\n",
    "    # 首先，我们需要将NER的结果转换成一个更方便处理的格式\n",
    "    entities = [{'word': d['word'], 'entity': d['entity'], 'score': d['score']} for d in ner_results]\n",
    "\n",
    "    # 然后，我们创建一个新的列表来存储处理后的实体\n",
    "    processed_entities = []\n",
    "    current_entity = []\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-') or (entity['entity'].startswith('I-') and not current_entity):\n",
    "            if current_entity:\n",
    "                processed_entities.append(current_entity)\n",
    "            current_entity = [entity]\n",
    "        elif entity['entity'].startswith('I-') and current_entity:\n",
    "            current_entity.append(entity)\n",
    "    if current_entity:\n",
    "        processed_entities.append(current_entity)\n",
    "\n",
    "    return processed_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T18:13:17.467939Z",
     "start_time": "2023-07-25T18:13:17.451123Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 定义一个字典储存每个国家的别名\n",
    "country_aliases = {\n",
    "\"United States\": [\"USA\", \"America\", \"US\", \"United States\"],\n",
    "\"Canada\": [\"Canada\", \"CA\"],\n",
    "\"United Kingdom\": [\"UK\", \"United Kingdom\", \"Britain\", \"England\", \"Scotland\", \"Wales\", \"Northern Ireland\"],\n",
    "\"Australia\": [\"Australia\", \"AU\", \"Aussie\"],\n",
    "\"China\": [\"China\", \"PRC\"],\n",
    "\"Denmark\": [\"Denmark\", \"DK\"],\n",
    "\"Finland\": [\"Finland\", \"FI\"],\n",
    "\"France\": [\"France\", \"French Republic\", \"FR\"],\n",
    "\"Germany\": [\"Germany\", \"DE\"],\n",
    "\"Japan\": [\"Japan\", \"JP\"],\n",
    "\"Italy\": [\"Italy\", \"Italian Republic\", \"IT\"],\n",
    "\"Netherlands\": [\"Netherlands\", \"Holland\", \"NL\"],\n",
    "\"Norway\": [\"Norway\", \"NO\"],\n",
    "\"Portugal\": [\"Portugal\", \"PT\"],\n",
    "\"Singapore\": [\"Singapore\", \"SG\"],\n",
    "\"South Korea\": [\"South Korea\", \"KR\"],\n",
    "\"Spain\": [\"Spain\", \"ES\"],\n",
    "\"Sweden\": [\"Sweden\", \"SE\"],\n",
    "\"Switzerland\": [\"Switzerland\", \"Swiss Confederation\", \"Swiss\", \"CH\"],\n",
    "\"New Zealand\": [\"New Zealand\", \"NZ\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T19:05:58.953964Z",
     "start_time": "2023-07-25T19:05:58.951539Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/ReutersArticles/United States_articles.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "Processing United States:   0%|          | 0/22907 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00cc488ebe034475a0446f4ec0c4a097"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_files = []\n",
    "\n",
    "Path = \"../Data/ReutersArticles/*.csv\"\n",
    "\n",
    "for fname in glob.glob(Path):\n",
    "    lst_files.append(fname)\n",
    "\n",
    "lst_files = sorted(lst_files)\n",
    "\n",
    "for file in lst_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    country_name = country_name.replace(\"_articles\", \"\")  # Remove the \"_articles\" part of the file name\n",
    "\n",
    "    # 仅对单个国家的数据进行实体识别\n",
    "    if file == '../Data/ReutersArticles/United States_articles.csv':\n",
    "    # 对所有的未进行过NER的国家进行实体识别\n",
    "    # if os.path.isfile('../Data/Reuters_NER/{0}.csv'.format(country_name)):\n",
    "    #     print(\"File already exists: {0}.csv\".format(country_name))\n",
    "    # else:\n",
    "        print(file)\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        if df.shape[0] != 0:\n",
    "            df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "            df = df.drop_duplicates(['Date','Headline'],keep= 'last')\n",
    "            df['Text'] = df['Text'].astype(str)\n",
    "            df['Text'] = df.apply(lambda row: combineHeadlineText(row), axis=1)\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.date  # change date format to YYYY-MM-DD\n",
    "            df = df.sort_values(by = ['Date'], ascending = True)\n",
    "\n",
    "            count = []\n",
    "            check = []\n",
    "            # 新建一个DataFrame储存实体识别结果\n",
    "            ner_results_data = pd.DataFrame()\n",
    "            for i in tqdm(range(len(df)), desc=f\"Processing {country_name}\"):\n",
    "                ner_results = nlp(df[\"Text\"].iloc[i])\n",
    "\n",
    "                processed_entities = process_entities(ner_results)\n",
    "\n",
    "                country_instances = []\n",
    "                country_check = []\n",
    "                aliases = country_aliases.get(country_name, [country_name])\n",
    "                for entity_group in processed_entities:\n",
    "                    words = [entity['word'] for entity in entity_group]\n",
    "                    entity_name = ' '.join(words)\n",
    "                    entity_type = entity_group[0]['entity']\n",
    "                    entity_score = sum(entity['score'] for entity in entity_group) / len(entity_group)\n",
    "                    if entity_type in [\"B-LOC\", \"B-ORG\"] and entity_score > 0.98:\n",
    "                        country_check.append(entity_name)\n",
    "                        if any(alias in entity_name for alias in aliases):\n",
    "                            country_instances.append(entity_name)\n",
    "\n",
    "                count.append(len(country_instances))\n",
    "                check.append(country_check)\n",
    "                ner_results_data = pd.concat([ner_results_data, pd.json_normalize(ner_results)], ignore_index=True)\n",
    "\n",
    "\n",
    "            df['Count'] = count\n",
    "            df_check = df.copy()\n",
    "            df_check['Check'] = check\n",
    "            df = df[df['Count'] > 0]\n",
    "            df = df.drop(['Count'], axis = 1)\n",
    "            df.to_csv(r'../Data/Reuters_NER/{0}.csv'.format(country_name))\n",
    "            # 添加df_check为csv文件, 并命名为country_name_check.csv\n",
    "            df_check.to_csv(r'../Data/NER_Entity/{0}_check.csv'.format(country_name))\n",
    "            ner_results_data.to_csv(r'../Data/NER_Entity/{0}.csv'.format(country_name), index=False)  # Save the ner_results DataFrame to a CSV file\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T19:36:22.954350Z",
     "start_time": "2023-07-25T19:12:37.704940Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netherlands 2661\n",
      "New Zealand 3171\n",
      "Singapore 2999\n",
      "Denmark 1675\n",
      "Italy 6109\n",
      "Norway 1998\n",
      "Japan 9954\n",
      "Finland 1329\n",
      "United States 17947\n",
      "United Kingdom 13011\n",
      "Germany 11026\n",
      "Canada 6950\n",
      "France 9304\n",
      "Portugal 1771\n",
      "Spain 4669\n",
      "Sweden 2202\n",
      "South Korea 3760\n",
      "Australia 9234\n",
      "Switzerland 2144\n",
      "China 20741\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家的实体识别后剩余的数据, 然后输出剩余数据的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/Reuters_NER/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "    print(country_name, df.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T19:36:32.856073Z",
     "start_time": "2023-07-25T19:36:30.302113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
