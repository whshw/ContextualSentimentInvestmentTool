{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:36.993142100Z",
     "start_time": "2023-08-14T14:59:35.648686100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# use the first GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 合并headline和text\n",
    "def combineHeadlineText(row):\n",
    "    if isinstance(row[\"Headline\"], str):\n",
    "        return row[\"Headline\"] + \". \" + row[\"Text\"]\n",
    "    else:\n",
    "        return row[\"Text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:37.958927400Z",
     "start_time": "2023-08-14T14:59:37.928687900Z"
    }
   },
   "id": "7f1759801abc8f19"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, use_parse=False):\n",
    "    df = df.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "    df = df.drop_duplicates(['Date', 'Headline'], keep='last')\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "    df['Text'] = df.apply(lambda row: combineHeadlineText(row), axis=1)\n",
    "    \n",
    "    if use_parse:\n",
    "        df['Date'] = df['Date'].str.replace(r'Published: ', ' ')\n",
    "        df['Date'] = df['Date'].str.replace(r'First', ' ')\n",
    "        df['Date'] = df['Date'].apply(lambda date_str: parse(date_str, tzinfos={'ET': gettz('America/New_York')}))\n",
    "        df['Date'] = df['Date'].dt.date\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).sort_values(by=['Date'], ascending=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:38.682412500Z",
     "start_time": "2023-08-14T14:59:38.660579700Z"
    }
   },
   "id": "350e69eed0d2c0f6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def process_entities(ner_results):\n",
    "    # 首先，我们需要将NER的结果转换成一个更方便处理的格式\n",
    "    entities = [{'word': d['word'], 'entity': d['entity'], 'score': d['score']} for d in ner_results]\n",
    "\n",
    "    # 然后，我们创建一个新的列表来存储处理后的实体\n",
    "    processed_entities = []\n",
    "    current_entity = []\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-') or (entity['entity'].startswith('I-') and not current_entity):\n",
    "            if current_entity:\n",
    "                processed_entities.append(current_entity)\n",
    "            current_entity = [entity]\n",
    "        elif entity['entity'].startswith('I-') and current_entity:\n",
    "            current_entity.append(entity)\n",
    "    if current_entity:\n",
    "        processed_entities.append(current_entity)\n",
    "\n",
    "    return processed_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:39.708448100Z",
     "start_time": "2023-08-14T14:59:39.689223500Z"
    }
   },
   "id": "404efce702a4e5a1"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def json_serializable(item):\n",
    "    \"\"\"Convert non-serializable items to serializable.\"\"\"\n",
    "    if isinstance(item, np.float32):\n",
    "        return float(item)\n",
    "    raise TypeError(f\"Type {type(item)} not serializable\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:40.493364900Z",
     "start_time": "2023-08-14T14:59:40.477402300Z"
    }
   },
   "id": "1196d0bb5de66018"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def perform_ner_on_dataframe(df, country_name):\n",
    "    count = []\n",
    "    check = []\n",
    "    ner_results_data_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(df)), desc=f\"Processing {country_name}\"):\n",
    "        ner_results = nlp(df[\"Text\"].iloc[i])\n",
    "        processed_entities = process_entities(ner_results)\n",
    "\n",
    "        country_instances = []\n",
    "        country_check = []\n",
    "        aliases = country_aliases.get(country_name, [country_name])\n",
    "        for entity_group in processed_entities:\n",
    "            words = [entity['word'] for entity in entity_group]\n",
    "            entity_name = ' '.join(words)\n",
    "            entity_type = entity_group[0]['entity']\n",
    "            entity_score = sum(entity['score'] for entity in entity_group) / len(entity_group)\n",
    "            if entity_type in [\"B-LOC\", \"B-ORG\"] and entity_score > 0.98:\n",
    "                country_check.append(entity_name)\n",
    "                if any(alias in entity_name for alias in aliases):\n",
    "                    country_instances.append(entity_name)\n",
    "\n",
    "        count.append(len(country_instances))\n",
    "        check.append(country_check)\n",
    "        \n",
    "        ner_result = {\n",
    "            'Date': df['Date'].iloc[i],\n",
    "            'Headline': df['Headline'].iloc[i],\n",
    "            'NER': json.dumps(ner_results, default=json_serializable)  # convert ner_results to string\n",
    "        }\n",
    "        ner_results_data_list.append(ner_result)\n",
    "\n",
    "    df['Count'] = count\n",
    "    df = df[df['Count'] >= 3]\n",
    "    df = df.drop(['Count'], axis=1)\n",
    "    df_ner_results = pd.DataFrame(ner_results_data_list)\n",
    "    \n",
    "    return df, df_ner_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:40.957206300Z",
     "start_time": "2023-08-14T14:59:40.936216900Z"
    }
   },
   "id": "8592be90b0ebcf55"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 定义一个字典储存每个国家的别名\n",
    "country_aliases = {\n",
    "\"UnitedStates\": [\"USA\", \"America\", \"US\", \"United States\", \"UnitedStates\"],\n",
    "\"Canada\": [\"Canada\", \"CA\"],\n",
    "\"UnitedKingdom\": [\"UK\", \"United Kingdom\", \"Britain\", \"England\", \"Scotland\", \"Wales\", \"Northern Ireland\", \"UnitedKingdom\"],\n",
    "\"Australia\": [\"Australia\", \"AU\", \"Aussie\"],\n",
    "\"China\": [\"China\", \"PRC\"],\n",
    "\"Denmark\": [\"Denmark\", \"DK\"],\n",
    "\"Finland\": [\"Finland\", \"FI\"],\n",
    "\"France\": [\"France\", \"French Republic\", \"FR\"],\n",
    "\"Germany\": [\"Germany\", \"DE\"],\n",
    "\"Japan\": [\"Japan\", \"JP\"],\n",
    "\"Italy\": [\"Italy\", \"Italian Republic\", \"IT\"],\n",
    "\"Netherlands\": [\"Netherlands\", \"Holland\", \"NL\"],\n",
    "\"Norway\": [\"Norway\", \"NO\"],\n",
    "\"Portugal\": [\"Portugal\", \"PT\"],\n",
    "\"Singapore\": [\"Singapore\", \"SG\"],\n",
    "\"SouthKorea\": [\"South Korea\", \"KR\", \"SouthKorea\"],\n",
    "\"Spain\": [\"Spain\", \"ES\"],\n",
    "\"Sweden\": [\"Sweden\", \"SE\"],\n",
    "\"Switzerland\": [\"Switzerland\", \"Swiss Confederation\", \"Swiss\", \"CH\"],\n",
    "\"NewZealand\": [\"New Zealand\", \"NZ\", \"NewZealand\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:59:41.569897300Z",
     "start_time": "2023-08-14T14:59:41.564738200Z"
    }
   },
   "id": "dc053fe0ef6b42c1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/CNNArticles\\China.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "Processing China:   0%|          | 0/15687 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2a6d3bd39314a7c8c9f5524449b85d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_files = []\n",
    "\n",
    "Path = \"../Data/CNNArticles/*.csv\"\n",
    "\n",
    "for fname in glob.glob(Path):\n",
    "    lst_files.append(fname)\n",
    "\n",
    "lst_files = sorted(lst_files)\n",
    "\n",
    "for file in lst_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "\n",
    "    # 仅对单个国家的数据进行实体识别\n",
    "    if country_name == \"China\":\n",
    "    # 对所有的未进行过NER的国家进行实体识别\n",
    "    # if os.path.isfile('../Data/CNN_NER/{0}.csv'.format(country_name)):\n",
    "    #     print(\"File already exists: {0}.csv\".format(country_name))\n",
    "    # else:\n",
    "        print(file)\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        if df.shape[0] != 0:\n",
    "            \n",
    "            df = preprocess_dataframe(df)\n",
    "\n",
    "            df, df_ner_results = perform_ner_on_dataframe(df, country_name)\n",
    "\n",
    "\n",
    "            # Convert the list of relevant articles back to a DataFrame\n",
    "            df.to_csv(r'../Data/CNN_NER/{0}.csv'.format(country_name))\n",
    "            df_ner_results.to_csv(r'../Data/CNN_NER_Results/{0}.csv'.format(country_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T15:35:02.959148800Z",
     "start_time": "2023-08-14T15:24:37.195296600Z"
    }
   },
   "id": "d41e4037f0c8234b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def merge_and_save_csv(file1, file2):\n",
    "    \"\"\"\n",
    "    Merge two csv files, remove duplicates, and save the merged data to the first file.\n",
    "\n",
    "    Parameters:\n",
    "    - file1 (str): Path to the first csv file. The merged data will be saved to this file.\n",
    "    - file2 (str): Path to the second csv file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取两个csv文件\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    \n",
    "    # 合并两个数据集\n",
    "    df = pd.concat([df1, df2], axis=0)\n",
    "    \n",
    "    # 去除重复部分\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # 保存合并后的数据到第一个文件\n",
    "    df.to_csv(file1, index=False)\n",
    "\n",
    "# 使用方法：\n",
    "# merge_and_save_csv(\"../Data/CNNArticles/UnitedStates.csv\", \"../Data/CNNarticles/UnitedStates2.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:43:33.074837500Z",
     "start_time": "2023-08-14T14:43:33.060333600Z"
    }
   },
   "id": "efc15aa872812867"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "file1 = \"../Data/CNNArticles/China.csv\"\n",
    "file2 = \"../Data/CNNArticles/China3.csv\"\n",
    "merge_and_save_csv(file1, file2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T14:45:41.126736200Z",
     "start_time": "2023-08-14T14:45:38.652904200Z"
    }
   },
   "id": "538e77d0b1286bb7"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 8605\n",
      "Canada 10939\n",
      "China 15695\n",
      "Denmark 2215\n",
      "Finland 1835\n",
      "France 9742\n",
      "Germany 9218\n",
      "Italy 8167\n",
      "Japan 9071\n",
      "Netherlands 3779\n",
      "NewZealand 4509\n",
      "Norway 2518\n",
      "Portugal 1815\n",
      "Singapore 4605\n",
      "SouthKorea 8595\n",
      "Spain 6164\n",
      "Sweden 3245\n",
      "Switzerland 3888\n",
      "UnitedKingdom 1462\n",
      "UnitedStates 28412\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家数据, 然后输出所得新闻的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/CNNArticles/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    print(country_name, df.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T15:42:10.673843200Z",
     "start_time": "2023-08-14T15:42:00.593956500Z"
    }
   },
   "id": "6d462eb50b937641"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 2016-05-05 2023-08-08\n",
      "Australia 1359\n",
      "Canada 2016-11-18 2023-08-02\n",
      "Canada 831\n",
      "China 2012-12-17 2023-08-11\n",
      "China 5494\n",
      "Denmark 2012-04-27 2023-08-01\n",
      "Denmark 136\n",
      "Finland 2012-05-26 2023-07-26\n",
      "Finland 127\n",
      "France 2019-06-07 2023-08-08\n",
      "France 474\n",
      "Germany 2017-07-06 2023-08-02\n",
      "Germany 706\n",
      "Italy 2013-06-17 2023-08-04\n",
      "Italy 762\n",
      "Japan 2016-11-03 2023-08-02\n",
      "Japan 1337\n",
      "Netherlands 2012-03-09 2023-07-28\n",
      "Netherlands 107\n",
      "NewZealand 2011-06-29 2023-08-11\n",
      "NewZealand 717\n",
      "Norway 2011-11-15 2023-07-06\n",
      "Norway 200\n",
      "Portugal 2011-09-28 2023-08-03\n",
      "Portugal 110\n",
      "Singapore 2011-09-26 2023-08-10\n",
      "Singapore 499\n",
      "SouthKorea 2013-03-26 2023-08-04\n",
      "SouthKorea 892\n",
      "Spain 2011-09-02 2023-08-11\n",
      "Spain 695\n",
      "Sweden 2011-10-29 2023-08-11\n",
      "Sweden 262\n",
      "Switzerland 2011-09-23 2023-08-08\n",
      "Switzerland 121\n",
      "UnitedKingdom 2022-06-07 2023-08-11\n",
      "UnitedKingdom 312\n",
      "UnitedStates 2011-04-05 2023-08-13\n",
      "UnitedStates 8138\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家的实体识别后剩余的数据, 然后输出剩余数据的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/CNN_NER/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "    # 展示每个国家的第一个新闻的日期和最后一个新闻的日期\n",
    "    print(country_name, df['Date'].iloc[0], df['Date'].iloc[-1])\n",
    "    print(country_name, df.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T15:48:31.473064Z",
     "start_time": "2023-08-14T15:48:30.564782300Z"
    }
   },
   "id": "efda418d2f2b2647"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [{'entity': 'B-MISC', 'score': 0.9997494816780...\n",
      "1       [{'entity': 'B-MISC', 'score': 0.9992812275886...\n",
      "2       [{'entity': 'B-MISC', 'score': 0.9922473430633...\n",
      "3       [{'entity': 'B-MISC', 'score': 0.9936847090721...\n",
      "4       [{'entity': 'B-LOC', 'score': 0.99973839521408...\n",
      "                              ...                        \n",
      "3769    [{'entity': 'B-PER', 'score': 0.93637478351593...\n",
      "3770    [{'entity': 'B-ORG', 'score': 0.94311094284057...\n",
      "3771    [{'entity': 'B-LOC', 'score': 0.99974805116653...\n",
      "3772    [{'entity': 'B-LOC', 'score': 0.99981027841567...\n",
      "3773    [{'entity': 'B-LOC', 'score': 0.99936670064926...\n",
      "Name: NER, Length: 3774, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 读取NER_Results数据, 然后输出所得新闻的数量\n",
    "path = \"../Data/CNN_NER_Results/Netherlands.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['NER'] = df['NER'].apply(json.loads)\n",
    "# 检查第一行的数据的NER列的第五行的数据\n",
    "print(df[\"NER\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:28:30.523922100Z",
     "start_time": "2023-08-12T17:28:29.993562600Z"
    }
   },
   "id": "79105b93b6f503d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9315c43d6d04c6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
