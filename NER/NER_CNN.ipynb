{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:21:42.352526Z",
     "start_time": "2023-08-12T17:21:41.170953700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# use the first GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 合并headline和text\n",
    "def combineHeadlineText(row):\n",
    "    if isinstance(row[\"Headline\"], str):\n",
    "        return row[\"Headline\"] + \". \" + row[\"Text\"]\n",
    "    else:\n",
    "        return row[\"Text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:06:54.075751100Z",
     "start_time": "2023-08-12T17:06:54.063239Z"
    }
   },
   "id": "7f1759801abc8f19"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, use_parse=False):\n",
    "    df = df.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "    df = df.drop_duplicates(['Date', 'Headline'], keep='last')\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "    df['Text'] = df.apply(lambda row: combineHeadlineText(row), axis=1)\n",
    "    \n",
    "    if use_parse:\n",
    "        df['Date'] = df['Date'].str.replace(r'Published: ', ' ')\n",
    "        df['Date'] = df['Date'].str.replace(r'First', ' ')\n",
    "        df['Date'] = df['Date'].apply(lambda date_str: parse(date_str, tzinfos={'ET': gettz('America/New_York')}))\n",
    "        df['Date'] = df['Date'].dt.date\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).sort_values(by=['Date'], ascending=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:06:54.659034800Z",
     "start_time": "2023-08-12T17:06:54.637555300Z"
    }
   },
   "id": "350e69eed0d2c0f6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_entities(ner_results):\n",
    "    # 首先，我们需要将NER的结果转换成一个更方便处理的格式\n",
    "    entities = [{'word': d['word'], 'entity': d['entity'], 'score': d['score']} for d in ner_results]\n",
    "\n",
    "    # 然后，我们创建一个新的列表来存储处理后的实体\n",
    "    processed_entities = []\n",
    "    current_entity = []\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-') or (entity['entity'].startswith('I-') and not current_entity):\n",
    "            if current_entity:\n",
    "                processed_entities.append(current_entity)\n",
    "            current_entity = [entity]\n",
    "        elif entity['entity'].startswith('I-') and current_entity:\n",
    "            current_entity.append(entity)\n",
    "    if current_entity:\n",
    "        processed_entities.append(current_entity)\n",
    "\n",
    "    return processed_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:06:55.547701300Z",
     "start_time": "2023-08-12T17:06:55.524313700Z"
    }
   },
   "id": "404efce702a4e5a1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def json_serializable(item):\n",
    "    \"\"\"Convert non-serializable items to serializable.\"\"\"\n",
    "    if isinstance(item, np.float32):\n",
    "        return float(item)\n",
    "    raise TypeError(f\"Type {type(item)} not serializable\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:23:35.970060200Z",
     "start_time": "2023-08-12T17:23:35.954080Z"
    }
   },
   "id": "1196d0bb5de66018"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def perform_ner_on_dataframe(df, country_name):\n",
    "    count = []\n",
    "    check = []\n",
    "    ner_results_data_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(df)), desc=f\"Processing {country_name}\"):\n",
    "        ner_results = nlp(df[\"Text\"].iloc[i])\n",
    "        processed_entities = process_entities(ner_results)\n",
    "\n",
    "        country_instances = []\n",
    "        country_check = []\n",
    "        aliases = country_aliases.get(country_name, [country_name])\n",
    "        for entity_group in processed_entities:\n",
    "            words = [entity['word'] for entity in entity_group]\n",
    "            entity_name = ' '.join(words)\n",
    "            entity_type = entity_group[0]['entity']\n",
    "            entity_score = sum(entity['score'] for entity in entity_group) / len(entity_group)\n",
    "            if entity_type in [\"B-LOC\", \"B-ORG\"] and entity_score > 0.98:\n",
    "                country_check.append(entity_name)\n",
    "                if any(alias in entity_name for alias in aliases):\n",
    "                    country_instances.append(entity_name)\n",
    "\n",
    "        count.append(len(country_instances))\n",
    "        check.append(country_check)\n",
    "        \n",
    "        ner_result = {\n",
    "            'Date': df['Date'].iloc[i],\n",
    "            'Headline': df['Headline'].iloc[i],\n",
    "            'NER': json.dumps(ner_results, default=json_serializable)  # convert ner_results to string\n",
    "        }\n",
    "        ner_results_data_list.append(ner_result)\n",
    "\n",
    "    df['Count'] = count\n",
    "    df = df[df['Count'] >= 3]\n",
    "    df = df.drop(['Count'], axis=1)\n",
    "    df_ner_results = pd.DataFrame(ner_results_data_list)\n",
    "    \n",
    "    return df, df_ner_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:23:49.270356800Z",
     "start_time": "2023-08-12T17:23:49.252328500Z"
    }
   },
   "id": "8592be90b0ebcf55"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 定义一个字典储存每个国家的别名\n",
    "country_aliases = {\n",
    "\"UnitedStates\": [\"USA\", \"America\", \"US\", \"United States\", \"UnitedStates\"],\n",
    "\"Canada\": [\"Canada\", \"CA\"],\n",
    "\"UnitedKingdom\": [\"UK\", \"United Kingdom\", \"Britain\", \"England\", \"Scotland\", \"Wales\", \"Northern Ireland\", \"UnitedKingdom\"],\n",
    "\"Australia\": [\"Australia\", \"AU\", \"Aussie\"],\n",
    "\"China\": [\"China\", \"PRC\"],\n",
    "\"Denmark\": [\"Denmark\", \"DK\"],\n",
    "\"Finland\": [\"Finland\", \"FI\"],\n",
    "\"France\": [\"France\", \"French Republic\", \"FR\"],\n",
    "\"Germany\": [\"Germany\", \"DE\"],\n",
    "\"Japan\": [\"Japan\", \"JP\"],\n",
    "\"Italy\": [\"Italy\", \"Italian Republic\", \"IT\"],\n",
    "\"Netherlands\": [\"Netherlands\", \"Holland\", \"NL\"],\n",
    "\"Norway\": [\"Norway\", \"NO\"],\n",
    "\"Portugal\": [\"Portugal\", \"PT\"],\n",
    "\"Singapore\": [\"Singapore\", \"SG\"],\n",
    "\"SouthKorea\": [\"South Korea\", \"KR\", \"SouthKorea\"],\n",
    "\"Spain\": [\"Spain\", \"ES\"],\n",
    "\"Sweden\": [\"Sweden\", \"SE\"],\n",
    "\"Switzerland\": [\"Switzerland\", \"Swiss Confederation\", \"Swiss\", \"CH\"],\n",
    "\"NewZealand\": [\"New Zealand\", \"NZ\", \"NewZealand\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:06:57.565224500Z",
     "start_time": "2023-08-12T17:06:57.544515500Z"
    }
   },
   "id": "dc053fe0ef6b42c1"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/CNNArticles\\Netherlands.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "Processing Netherlands:   0%|          | 0/3774 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "918952fda18648ab9290712639365161"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_files = []\n",
    "\n",
    "Path = \"../Data/CNNArticles/*.csv\"\n",
    "count = 0\n",
    "\n",
    "for fname in glob.glob(Path):\n",
    "    lst_files.append(fname)\n",
    "\n",
    "lst_files = sorted(lst_files)\n",
    "\n",
    "for file in lst_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "\n",
    "    if not os.path.isfile('../Data/CNN_NER/{0}.csv'.format(country_name)):\n",
    "        print(file)\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        if df.shape[0] != 0:\n",
    "            \n",
    "            df = preprocess_dataframe(df)\n",
    "\n",
    "            df, df_ner_results = perform_ner_on_dataframe(df, country_name)\n",
    "\n",
    "\n",
    "            # Convert the list of relevant articles back to a DataFrame\n",
    "            df.to_csv(r'../Data/CNN_NER/{0}.csv'.format(country_name))\n",
    "            df_ner_results.to_csv(r'../Data/CNN_NER_Results/{0}.csv'.format(country_name))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:25:55.740031600Z",
     "start_time": "2023-08-12T17:23:51.031779400Z"
    }
   },
   "id": "d41e4037f0c8234b"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# 读取UnitedStates和UnitedStates2的数据，合并为UnitedStates\n",
    "df1 = pd.read_csv(\"../Data/CNNArticles/UnitedStates.csv\")\n",
    "df2 = pd.read_csv(\"../Data/CNNarticles/UnitedStates2.csv\")\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "# 保存到UnitedStates.csv\n",
    "df.to_csv(r'../Data/CNNArticles/UnitedStates.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T14:06:10.436598100Z",
     "start_time": "2023-08-12T14:06:08.108511400Z"
    }
   },
   "id": "efc15aa872812867"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 8605\n",
      "Canada 10939\n",
      "China 9229\n",
      "Denmark 2215\n",
      "Finland 1835\n",
      "France 9742\n",
      "Germany 9218\n",
      "Italy 8167\n",
      "Japan 9071\n",
      "Netherlands 3779\n",
      "NewZealand 4509\n",
      "Norway 2518\n",
      "Portugal 1815\n",
      "Singapore 4605\n",
      "SouthKorea 8595\n",
      "Spain 6164\n",
      "Sweden 3245\n",
      "Switzerland 3888\n",
      "UnitedKingdom 1462\n",
      "UnitedStates 18586\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家数据, 然后输出所得新闻的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/CNNArticles/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    print(country_name, df.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T14:10:11.405247300Z",
     "start_time": "2023-08-12T14:10:02.122637100Z"
    }
   },
   "id": "6d462eb50b937641"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 1359\n",
      "Canada 831\n",
      "China 2833\n",
      "Denmark 136\n",
      "Finland 127\n",
      "France 474\n",
      "Germany 706\n",
      "Italy 762\n",
      "Japan 1337\n",
      "Netherlands 107\n",
      "NewZealand 717\n",
      "Norway 200\n",
      "Portugal 110\n",
      "Singapore 499\n",
      "SouthKorea 892\n",
      "Spain 695\n",
      "Sweden 262\n",
      "Switzerland 121\n",
      "UnitedKingdom 312\n",
      "UnitedStates 5971\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家的实体识别后剩余的数据, 然后输出剩余数据的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/CNN_NER/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "    print(country_name, df.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:26:57.156804100Z",
     "start_time": "2023-08-12T17:26:56.179737100Z"
    }
   },
   "id": "efda418d2f2b2647"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [{'entity': 'B-MISC', 'score': 0.9997494816780...\n",
      "1       [{'entity': 'B-MISC', 'score': 0.9992812275886...\n",
      "2       [{'entity': 'B-MISC', 'score': 0.9922473430633...\n",
      "3       [{'entity': 'B-MISC', 'score': 0.9936847090721...\n",
      "4       [{'entity': 'B-LOC', 'score': 0.99973839521408...\n",
      "                              ...                        \n",
      "3769    [{'entity': 'B-PER', 'score': 0.93637478351593...\n",
      "3770    [{'entity': 'B-ORG', 'score': 0.94311094284057...\n",
      "3771    [{'entity': 'B-LOC', 'score': 0.99974805116653...\n",
      "3772    [{'entity': 'B-LOC', 'score': 0.99981027841567...\n",
      "3773    [{'entity': 'B-LOC', 'score': 0.99936670064926...\n",
      "Name: NER, Length: 3774, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 读取NER_Results数据, 然后输出所得新闻的数量\n",
    "path = \"../Data/CNN_NER_Results/Netherlands.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['NER'] = df['NER'].apply(json.loads)\n",
    "# 检查第一行的数据的NER列的第五行的数据\n",
    "print(df[\"NER\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T17:28:30.523922100Z",
     "start_time": "2023-08-12T17:28:29.993562600Z"
    }
   },
   "id": "79105b93b6f503d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9315c43d6d04c6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
