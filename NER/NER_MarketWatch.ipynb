{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries\n",
    "Please note that the device setup need to fit your system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:40:19.696295800Z",
     "start_time": "2023-08-16T08:40:14.429526100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from   os import path\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# use the first GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = torch.device(\"mps\"if torch.backends.mps.is_available()else \"cpu\")\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def combineHeadlineText(row):\n",
    "    if isinstance(row[\"Headline\"], str):\n",
    "        return row[\"Headline\"] + \". \" + row[\"Text\"]\n",
    "    else:\n",
    "        return row[\"Text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:40:19.713861900Z",
     "start_time": "2023-08-16T08:40:19.696295800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, use_parse=False):\n",
    "    df = df.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
    "    df = df.drop_duplicates(['Date', 'Headline'], keep='last')\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "    df['Text'] = df.apply(lambda row: combineHeadlineText(row), axis=1)\n",
    "    \n",
    "    if use_parse:\n",
    "        df['Date'] = df['Date'].str.replace(r'Published: ', ' ')\n",
    "        df['Date'] = df['Date'].str.replace(r'First', ' ')\n",
    "        df['Date'] = df['Date'].apply(lambda date_str: parse(date_str, tzinfos={'ET': gettz('America/New_York')}))\n",
    "        df['Date'] = df['Date'].dt.date\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    df = df.reset_index(drop=True).sort_values(by=['Date'], ascending=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:07.916347300Z",
     "start_time": "2023-08-16T08:42:07.908347500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_entities(ner_results):\n",
    "    # 首先，我们需要将NER的结果转换成一个更方便处理的格式\n",
    "    entities = [{'word': d['word'], 'entity': d['entity'], 'score': d['score']} for d in ner_results]\n",
    "\n",
    "    # 然后，我们创建一个新的列表来存储处理后的实体\n",
    "    processed_entities = []\n",
    "    current_entity = []\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-') or (entity['entity'].startswith('I-') and not current_entity):\n",
    "            if current_entity:\n",
    "                processed_entities.append(current_entity)\n",
    "            current_entity = [entity]\n",
    "        elif entity['entity'].startswith('I-') and current_entity:\n",
    "            current_entity.append(entity)\n",
    "    if current_entity:\n",
    "        processed_entities.append(current_entity)\n",
    "\n",
    "    return processed_entities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:08.995529600Z",
     "start_time": "2023-08-16T08:42:08.986530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def json_serializable(item):\n",
    "    \"\"\"Convert non-serializable items to serializable.\"\"\"\n",
    "    if isinstance(item, np.float32):\n",
    "        return float(item)\n",
    "    raise TypeError(f\"Type {type(item)} not serializable\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:10.153911300Z",
     "start_time": "2023-08-16T08:42:10.114859100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def perform_ner_on_dataframe(df, country_name):\n",
    "    count = []\n",
    "    check = []\n",
    "    ner_results_data_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(df)), desc=f\"Processing {country_name}\"):\n",
    "        ner_results = nlp(df[\"Text\"].iloc[i])\n",
    "        processed_entities = process_entities(ner_results)\n",
    "\n",
    "        country_instances = []\n",
    "        country_check = []\n",
    "        aliases = country_aliases.get(country_name, [country_name])\n",
    "        for entity_group in processed_entities:\n",
    "            words = [entity['word'] for entity in entity_group]\n",
    "            entity_name = ' '.join(words)\n",
    "            entity_type = entity_group[0]['entity']\n",
    "            entity_score = sum(entity['score'] for entity in entity_group) / len(entity_group)\n",
    "            if entity_type in [\"B-LOC\", \"B-ORG\"] and entity_score > 0.98:\n",
    "                country_check.append(entity_name)\n",
    "                if any(alias in entity_name for alias in aliases):\n",
    "                    country_instances.append(entity_name)\n",
    "\n",
    "        count.append(len(country_instances))\n",
    "        check.append(country_check)\n",
    "        \n",
    "        ner_result = {\n",
    "            'Date': df['Date'].iloc[i],\n",
    "            'Headline': df['Headline'].iloc[i],\n",
    "            'NER': json.dumps(ner_results, default=json_serializable)  # convert ner_results to string\n",
    "        }\n",
    "        ner_results_data_list.append(ner_result)\n",
    "\n",
    "    df['Count'] = count\n",
    "    df = df[df['Count'] >= 3]\n",
    "    df = df.drop(['Count'], axis=1)\n",
    "    df_ner_results = pd.DataFrame(ner_results_data_list)\n",
    "    \n",
    "    return df, df_ner_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:10.489895700Z",
     "start_time": "2023-08-16T08:42:10.471387900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 定义一个字典储存每个国家的别名\n",
    "country_aliases = {\n",
    "\"United States\": [\"USA\", \"America\", \"US\", \"United States\", \"UnitedStates\"],\n",
    "\"Canada\": [\"Canada\", \"CA\"],\n",
    "\"United Kingdom\": [\"UK\", \"United Kingdom\", \"Britain\", \"England\", \"Scotland\", \"Wales\", \"Northern Ireland\", \"UnitedKingdom\"],\n",
    "\"Australia\": [\"Australia\", \"AU\", \"Aussie\"],\n",
    "\"China\": [\"China\", \"PRC\"],\n",
    "\"Denmark\": [\"Denmark\", \"DK\"],\n",
    "\"Finland\": [\"Finland\", \"FI\"],\n",
    "\"France\": [\"France\", \"French Republic\", \"FR\"],\n",
    "\"Germany\": [\"Germany\", \"DE\"],\n",
    "\"Japan\": [\"Japan\", \"JP\"],\n",
    "\"Italy\": [\"Italy\", \"Italian Republic\", \"IT\"],\n",
    "\"Netherlands\": [\"Netherlands\", \"Holland\", \"NL\"],\n",
    "\"Norway\": [\"Norway\", \"NO\"],\n",
    "\"Portugal\": [\"Portugal\", \"PT\"],\n",
    "\"Singapore\": [\"Singapore\", \"SG\"],\n",
    "\"South Korea\": [\"South Korea\", \"KR\", \"SouthKorea\"],\n",
    "\"Spain\": [\"Spain\", \"ES\"],\n",
    "\"Sweden\": [\"Sweden\", \"SE\"],\n",
    "\"Switzerland\": [\"Switzerland\", \"Swiss Confederation\", \"Swiss\", \"CH\"],\n",
    "\"New Zealand\": [\"New Zealand\", \"NZ\", \"NewZealand\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:11.660110200Z",
     "start_time": "2023-08-16T08:42:11.638023100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Articles/MWArticles\\China_articles.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "Processing China:   0%|          | 0/59958 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "420eab2fa0944d988bc2fa75496523e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst_files = []\n",
    "\n",
    "Path = \"../Data/Articles/MWArticles/*.csv\"\n",
    "\n",
    "for fname in glob.glob(Path):\n",
    "    lst_files.append(fname)\n",
    "\n",
    "lst_files = sorted(lst_files)\n",
    "\n",
    "for file in lst_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    country_name = country_name.replace(\"_articles\", \"\")  # Remove the \"_articles\" part of the file name\n",
    "\n",
    "\n",
    "    # 仅对单个国家的数据进行实体识别\n",
    "    if country_name == \"China\":\n",
    "    # 对所有的未进行过NER的国家进行实体识别\n",
    "    # if os.path.isfile('../Data/MW_NER/{0}.csv'.format(country_name)):\n",
    "    #     print(\"File already exists: {0}.csv\".format(country_name))\n",
    "    # else:\n",
    "        print(file)\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        if df.shape[0] != 0:\n",
    "            df = preprocess_dataframe(df, use_parse=True)\n",
    "\n",
    "            df, df_ner_results = perform_ner_on_dataframe(df, country_name)\n",
    "\n",
    "            # Convert the list of relevant articles back to a DataFrame\n",
    "            df.to_csv(r'../Data/MW_NER/{0}.csv'.format(country_name))\n",
    "            df_ner_results.to_csv(r'../Data/MW_NER_Results/{0}.csv'.format(country_name))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-16T08:42:55.776620300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia 1989\n",
      "Canada 1069\n",
      "China 15272\n",
      "Denmark 75\n",
      "Finland 64\n",
      "France 1055\n",
      "Germany 1102\n",
      "Italy 828\n",
      "Japan 2660\n",
      "Netherlands 66\n",
      "New Zealand 279\n",
      "Norway 165\n",
      "Portugal 125\n",
      "Singapore 362\n",
      "South Korea 395\n",
      "Spain 991\n",
      "Sweden 150\n",
      "Switzerland 173\n",
      "United Kingdom 217\n",
      "United States 1044\n"
     ]
    }
   ],
   "source": [
    "# 读取所有国家的实体识别后剩余的数据, 然后输出剩余数据的数量\n",
    "lst_ner_files = []\n",
    "for fname in glob.glob(\"../Data/NER/MW_NER/*.csv\"):\n",
    "    lst_ner_files.append(fname)\n",
    "for file in lst_ner_files:\n",
    "    file_name = os.path.basename(file)  # Get the file name from the full file path\n",
    "    country_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    df = pd.read_csv(file)\n",
    "    print(country_name, df.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:42:44.561671800Z",
     "start_time": "2023-08-16T08:42:43.712145600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/countries_integration/Denmark_articles.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "Processing Denmark:   0%|          | 0/1412 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f30c31ddbb7465ab032816cd845d3a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_single_article(text, country_name):\n",
    "    ner_results = nlp(text)\n",
    "    country_instances = [d for d in ner_results if (d['entity'] in \"B-ORG\") and (d['word'] in country_name) and (d['score'] > 0.98)]\n",
    "    return len(country_instances)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-25T13:33:44.315833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412\n",
      "839\n"
     ]
    }
   ],
   "source": [
    "# Find all csv files in the path\n",
    "csv_files = glob.glob('../Data/countries_integration/*.csv')\n",
    "\n",
    "# Sort the file names\n",
    "csv_files = sorted(csv_files)\n",
    "\n",
    "# Read the first csv file\n",
    "df = pd.read_csv(csv_files[0])\n",
    "\n",
    "# Get the first row of the DataFrame\n",
    "first_row = df.iloc[43]\n",
    "\n",
    "# Extract the 'Date', 'Headline' and 'Text' columns\n",
    "date = first_row['Date']\n",
    "headline = first_row['Headline']\n",
    "text = first_row['Text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-25T13:38:34.826715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'B-ORG',\n  'score': 0.9932053,\n  'index': 1,\n  'word': 'AN',\n  'start': 0,\n  'end': 2},\n {'entity': 'I-ORG',\n  'score': 0.992149,\n  'index': 2,\n  'word': '##Z',\n  'start': 2,\n  'end': 3},\n {'entity': 'B-ORG',\n  'score': 0.99892753,\n  'index': 4,\n  'word': 'RB',\n  'start': 9,\n  'end': 11},\n {'entity': 'I-ORG',\n  'score': 0.9988914,\n  'index': 5,\n  'word': '##A',\n  'start': 11,\n  'end': 12},\n {'entity': 'B-ORG',\n  'score': 0.9988292,\n  'index': 20,\n  'word': 'Australia',\n  'start': 56,\n  'end': 65},\n {'entity': 'I-ORG',\n  'score': 0.99927264,\n  'index': 21,\n  'word': '&',\n  'start': 66,\n  'end': 67},\n {'entity': 'I-ORG',\n  'score': 0.9993352,\n  'index': 22,\n  'word': 'New',\n  'start': 68,\n  'end': 71},\n {'entity': 'I-ORG',\n  'score': 0.9992262,\n  'index': 23,\n  'word': 'Zealand',\n  'start': 72,\n  'end': 79},\n {'entity': 'I-ORG',\n  'score': 0.999316,\n  'index': 24,\n  'word': 'Banking',\n  'start': 80,\n  'end': 87},\n {'entity': 'I-ORG',\n  'score': 0.99929905,\n  'index': 25,\n  'word': 'Group',\n  'start': 88,\n  'end': 93},\n {'entity': 'I-ORG',\n  'score': 0.99907744,\n  'index': 26,\n  'word': 'Ltd',\n  'start': 94,\n  'end': 97},\n {'entity': 'B-ORG',\n  'score': 0.99583226,\n  'index': 29,\n  'word': 'AN',\n  'start': 100,\n  'end': 102},\n {'entity': 'I-ORG',\n  'score': 0.98822916,\n  'index': 30,\n  'word': '##Z',\n  'start': 102,\n  'end': 103},\n {'entity': 'I-ORG',\n  'score': 0.5856561,\n  'index': 31,\n  'word': '.',\n  'start': 103,\n  'end': 104},\n {'entity': 'I-ORG',\n  'score': 0.99690634,\n  'index': 32,\n  'word': 'AU',\n  'start': 104,\n  'end': 106},\n {'entity': 'B-MISC',\n  'score': 0.9997466,\n  'index': 38,\n  'word': 'Australian',\n  'start': 133,\n  'end': 143},\n {'entity': 'B-LOC',\n  'score': 0.9997409,\n  'index': 65,\n  'word': 'Australia',\n  'start': 273,\n  'end': 282},\n {'entity': 'B-ORG',\n  'score': 0.99879766,\n  'index': 78,\n  'word': 'Reserve',\n  'start': 336,\n  'end': 343},\n {'entity': 'I-ORG',\n  'score': 0.9993382,\n  'index': 79,\n  'word': 'Bank',\n  'start': 344,\n  'end': 348},\n {'entity': 'I-ORG',\n  'score': 0.9992987,\n  'index': 80,\n  'word': 'of',\n  'start': 349,\n  'end': 351},\n {'entity': 'I-ORG',\n  'score': 0.99930394,\n  'index': 81,\n  'word': 'Australia',\n  'start': 352,\n  'end': 361},\n {'entity': 'B-MISC',\n  'score': 0.9997414,\n  'index': 114,\n  'word': 'Australian',\n  'start': 491,\n  'end': 501},\n {'entity': 'B-ORG',\n  'score': 0.9984345,\n  'index': 126,\n  'word': 'RB',\n  'start': 553,\n  'end': 555},\n {'entity': 'I-ORG',\n  'score': 0.9982514,\n  'index': 127,\n  'word': '##A',\n  'start': 555,\n  'end': 556},\n {'entity': 'B-ORG',\n  'score': 0.9973816,\n  'index': 169,\n  'word': 'AN',\n  'start': 770,\n  'end': 772},\n {'entity': 'I-ORG',\n  'score': 0.99851054,\n  'index': 170,\n  'word': '##Z',\n  'start': 772,\n  'end': 773},\n {'entity': 'B-LOC',\n  'score': 0.9995554,\n  'index': 184,\n  'word': 'Australia',\n  'start': 835,\n  'end': 844},\n {'entity': 'B-ORG',\n  'score': 0.9981427,\n  'index': 235,\n  'word': 'RB',\n  'start': 1089,\n  'end': 1091},\n {'entity': 'I-ORG',\n  'score': 0.9981115,\n  'index': 236,\n  'word': '##A',\n  'start': 1091,\n  'end': 1092},\n {'entity': 'B-ORG',\n  'score': 0.9991461,\n  'index': 291,\n  'word': 'Goldman',\n  'start': 1392,\n  'end': 1399},\n {'entity': 'I-ORG',\n  'score': 0.99940324,\n  'index': 292,\n  'word': 'Sachs',\n  'start': 1400,\n  'end': 1405},\n {'entity': 'I-ORG',\n  'score': 0.99936825,\n  'index': 293,\n  'word': 'Group',\n  'start': 1406,\n  'end': 1411},\n {'entity': 'I-ORG',\n  'score': 0.99941456,\n  'index': 294,\n  'word': 'Inc',\n  'start': 1412,\n  'end': 1415},\n {'entity': 'B-ORG',\n  'score': 0.98708224,\n  'index': 299,\n  'word': 'G',\n  'start': 1420,\n  'end': 1421},\n {'entity': 'I-ORG',\n  'score': 0.913189,\n  'index': 300,\n  'word': '##S',\n  'start': 1421,\n  'end': 1422},\n {'entity': 'B-PER',\n  'score': 0.99957937,\n  'index': 307,\n  'word': 'Tim',\n  'start': 1450,\n  'end': 1453},\n {'entity': 'I-PER',\n  'score': 0.99967384,\n  'index': 308,\n  'word': 'Too',\n  'start': 1454,\n  'end': 1457},\n {'entity': 'I-PER',\n  'score': 0.9991948,\n  'index': 309,\n  'word': '##hey',\n  'start': 1457,\n  'end': 1460},\n {'entity': 'B-MISC',\n  'score': 0.99970394,\n  'index': 335,\n  'word': 'Australian',\n  'start': 1563,\n  'end': 1573},\n {'entity': 'B-ORG',\n  'score': 0.99665314,\n  'index': 364,\n  'word': 'AN',\n  'start': 1748,\n  'end': 1750},\n {'entity': 'I-ORG',\n  'score': 0.9985342,\n  'index': 365,\n  'word': '##Z',\n  'start': 1750,\n  'end': 1751},\n {'entity': 'B-PER',\n  'score': 0.9965178,\n  'index': 371,\n  'word': 'Caroline',\n  'start': 1766,\n  'end': 1774},\n {'entity': 'I-PER',\n  'score': 0.99918824,\n  'index': 372,\n  'word': 'He',\n  'start': 1775,\n  'end': 1777},\n {'entity': 'I-PER',\n  'score': 0.8371813,\n  'index': 373,\n  'word': '##ns',\n  'start': 1777,\n  'end': 1779},\n {'entity': 'I-PER',\n  'score': 0.9167949,\n  'index': 374,\n  'word': '##haw',\n  'start': 1779,\n  'end': 1782}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-22T16:17:52.976153Z",
     "start_time": "2023-07-22T16:17:52.970590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
